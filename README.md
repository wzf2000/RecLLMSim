# To What Extent Can LLMs Assist in User Simulation for Task-Oriented Conversations

## Data

We provide all the data files under the `real_human_user` and `LLM_simulated` folder.

The conversation dataset is divided into several parts according to the task and the user. Each conversation is stored in a `.json` file under the corresponding task folder.

The format of the `.json` file is:
```json
{
    "task": "The name of the task for this conversation.",
    "preference_id": "The ID of the user profile used in this conversation.",
    "task_context_id": "The ID of the task description used in this conversation.",
    "preference": "Preference used in this conversation.",
    "preference_zh": "Preference used in this conversation translated in Chinese.",
    "task_context": "The scenario-specific task description used in this conversation",
    "task_context_zh": "The scenario-specific task description used in this conversation translated in Chinese.",
    "history": [
        {
            "role": "user",
            "content": "Some text generated by user simulator LLM.",
            "content_zh": "Some text generated by user simulator LLM translated in Chinese.",
            "intent": "The intent annotation of the user simulator LLM.",
            "intent_zh": "The intent annotation of the user simulator LLM translated in Chinese.",
        },
        {
            "role": "assistant",
            "content": "Some text generated by assistant LLM.",
            "content_zh": "Some text generated by assistant LLM translated in Chinese.",
            "hallucination": {
                "hallucination": "Whether the assistant hallucinates in this turn.",
                "memo": "The memo of the hallucination.",
            },
        },
        // ...
    ],
    "conflict": false,
    "preference_summary": "The summary of the user simulator's preference extracted from the conversation.",
    "rating": {
        // Some rating information.
    },
    // Some other information.
}
```

## User Simulator Framework

We provide the code for the framework in the `RecUserSim` folder.

Check the `readme.md` file in the `RecUserSim` folder for more details.

## File Structure

- `real_human_user`: The conversation data collected from real human users.
- `LLM_simulated`: The conversation data generated by the user simulator LLM.
- `RecUserSim`: The code for the user simulator framework.
- `README.md`: This file.
- `conversational_strategy`: Automatically label method for conversational strategy.
- `profile_reconstruct`: The code for our profile reconstruction experiments.
- `statistics`: The code for the statistics of the dataset.

## Some Statistics

### Human Annotation for Conversational Strategy

We sample some conversations from the dataset and manually annotate the conversational strategy.

The consistency between the manual annotation and the automatic annotation is as follows:

|  **strategy**  | **Information request** | **Context dependency** | **Question** | **Depth Vs. Breadth** | **Feedback** |
| :------------: | :---------------------: | :--------------------: | :----------: | :-------------------: | :----------: |
| **Match rate** |           0.9           |          0.9           |      1       |           1           |     0.6      |

### Simulation Quality across Different Conversation Turns

| Turns | # Conv. | Preference Alignment | Role-Playing Completeness |
| :---: | :-----: | :------------------: | :-----------------------: |
|   1   |    1    |        1.0000        |          1.0000           |
|   2   |   316   |        1.2089        |          1.2911           |
|   3   |   575   |        1.3496        |          1.6417           |
|   4   |   452   |        1.6018        |          1.6416           |
|   5   |   279   |        1.7097        |          1.6093           |
|   6   |   138   |        1.8043        |          1.7246           |
|   7   |   69    |        1.7536        |          1.7101           |
|   8   |   15    |      **1.8667**      |        **1.7333**         |
|   9   |    6    |        1.6667        |          1.3333           |
|  10   |    5    |        1.6000        |          1.2000           |
